# Cmake to see if this works (CPU)
cd /Users/brianjohnson/VA_IBD/llama.cpp
git checkout brian-features
make 

# Llama3 8b calYear testing postAnswer & system prompt variation
./ibd_hx_type \
-m ~/Downloads/Meta-Llama-3-8B-Instruct_F16_brianConverted.gguf \
--sequences 38 --parallel 1 --cont-batching --n-predict 30 --batch-size 2048 --threads 4 --ctx-size 20000 \
--no-escape \
--n-gpu-layers 99 \
--temp 0 \
--patientFile /Users/brianjohnson/VA_IBD/testing_data/IBD_hx_deID/concat_patientIDs_06042024.txt \
--outDir ../llm_ibd_outDir/llama8 \
--grammar-file ./grammars/ibd_hx_type.gbnf \
--promptFormat llama3 \
--file /Users/brianjohnson/VA_IBD/testing_data/IBD_hx_deID/concat_input_06062024_revChronological.txt